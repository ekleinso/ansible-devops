---
- name: gpt-neo
  gather_facts: no
  hosts: localhost
  vars:
    syom_hf_token: "<your hugging face token>"
    syom_model_name: "EleutherAI/gpt-neo-125m"
    syom_description: "GPT-Neo 125M is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 125M represents the number of parameters of this particular pre-trained model."
    syom_description_short: "GPT-Neo 125M is a transformer model designed using EleutherAI's replication of the GPT-3 architecture."
    syom_source: "Hugging Face"
    syom_deployment_framework: "hf_transformers"
    syom_flash_attention: "false"
    syom_number_params: "125m"
    syom_project: "ibm-cpd-instance"
    syom_pvc_size: "20Gi"
    syom_pvc_sc: "managed-nfs-storage"

  roles:
    - ibmce.ocp_ansible.cp4d_syom

